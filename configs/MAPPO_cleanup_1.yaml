defaults:
  - hydra: hydra_simple
  - _self_


name: CookingZoo
separate_controllers: false

logger:
  load_dir: ${name}
  log_grad: false
  project: DiversityGeneration
  entity: harshg5799
  logger_period: 0.1
  save_model: true
  save_model_period: 5

# # Data collection-related parameters
env:
  name: cleanup-game-mini-reducedaction-fullycooperative-v0
  parallel:
    sp_collection: 30
    xp_collection: 0
    eval: 8
  ENV_DESCRIPTION : "Clean up is a public goods dilemma in which agents get a reward for consuming apples, but must use a cleaning beam to clean a river in order for apples to grow. While an agent is cleaning the river, other agents can exploit it by consuming the apples that appear."
  INSTRUCTION : "The environment is a 7 x 7 grid which comprises water and apples and 2 agents. Environment is indexed from 0-6. Apples can grow in positions [[5,0],[5,1],[5,2],[5,3],[5,4],[5,5],[5,6],[6,0],[6,1],[6,2],[6,3],[6,4],[6,5],[6,6]]. Water is in [[0,0],[0,1],[0,2],[1,0],[1,1],[2,0],[2,1],[3,0]]. The agents can move in the 4 cardinal directions. The agents can consume apples and clean the river. The agents must learn to cooperate to maximize their reward. Your taks is to encode the given trajectory that is obtained from a policy model for each agent. The trajectory is in the format : (Step No.) - Agent 1 (x,y,orientation(NSEW),action,reward) - Agent 2 (x,y,orientation,action,reward) - Clean Water[(x,y)] - Unclean Water [(x,y)] - Apples[(x,y)]. Note that Clean Water, Unclean Water and Apples are a list of coordinates. The trajectory is a list of such steps."

run:
  use_cuda: false
  num_timesteps: 10000000
  num_eval_episodes: 1
  seed: 4
  eval_seed: 500
  total_checkpoints: 1000
  load_from_checkpoint: -1
  device_id: 2
  cpus_per_task: 10

populations:
  num_populations: 1

train:
  method: FCP
  timesteps_per_update: 100
  gamma: 0.99
  gamma_act_jsd: 0.4
  target_update_rate: 1e-3
  lr: 1e-4
  max_grad_norm: 1.0
  epochs_per_update: 8
  eps_clip: 0.2
  dual_clip: 3.0
  with_dual_clip: false

loss_weights:
  sp_val_loss_weight: 1.0
  xp_loss_weights: 0.0 
  entropy_regularizer_loss: 0.001
  jsd_weight: 1

exploration_reward_weight: -0.0
  
model:
  actor_dims:
    - 128
    - 256
    - 256
    - 128
  critic_dims:
    - 128
    - 256
    - 256
    - 128
  init_ortho: false

buffer:
  type: Buffer # Buffer or LLMBuffer
  max_episodes: 10000